# 1. LEXING

---

## 1.1. Lexical Analysis

- change representation "two times" before the evaluation:

```note
source code -> tokens -> AST
```

- 1st transformation: sourcecode to token
  - "lexical analysis"
  - done by lexer a.k.a. tokenizer or scanner
  
- tokens
  - categorisable data structures that are then fed to the parser
    - parser does the 2nd transformation: tokens to AST

```gopher
"let x = 5 + 5;"
``` 

to lexer:

```gopher
[
    LET,
    IDENTIFIER("x"),
    EQUAL_SIGN,
    INTEGER(5),
    PLUS_SIGN,
    INTEGER(5),
    SEMICOLON
]
```

so you can see from the each token of the code is represented as:

- `let` -> `LET`
- `x` -> `IDENTIFIER("x")`
- `=` -> `EQUAL_SIGN`
- `5` -> `INTEGER(5)`
- `+` -> `PLUS_SIGN`
- `;` -> `SEMICOLON`

note, whitespaces, ` `, does not show up as a token. it acts as a separator for other tokens. therefore:

```gopher
let      x    =    5;
```
 
will be considered same as 

```gopher
let x = 5;
```

some other languages like, `python`, the length of whitespaces is significant,
lexer can't eat up these whitespaces and newline characters.
this means `python` consider whitespaces and newlines as tokens.

- production-ready lexer might attach 
  - line number
  - column number 
  - filename
- to token

why? output error message can be more useful, instead of:

```bash
"error: expected semicolon token"
```

better idea:

```bash
"error: expected semicolon token. line 42, column 23, program.gopher"
```

---

## 1.2. Defining Our Tokens

- todo:
  - define the tokens
  - so that our lexer can outputs the result
  
```gopher
let five = 5;
let ten = 10;

let add = fn(x, y) {
    x + y;
};

let result = add (five, ten);
```

breaking down of the code above:

- numbers like `5` and `10`
- variable names like `x`, `y`, `add`, `result`
- part of language, keywords likes `let`, `fn`
- special characters like `(`, `)`, `{`, `}`, `=`, `,`, `;`

numbers, we can consider as a special type, lexer don't care whether it's `5` or `10`,
it just needs to know that's number or not. same idea goes for "variable names". there are called as:

> identifiers
 
and treat these the same.

the other words, like `fn`, `let` we don't group them as "identifiers", we call them as "keywords",
which is a part of the language.
this is because, it should make a different in parsing stage when we encounter `fn` or `let`.
same for the special characters.

example of token data structure: [token/token.go](../gopher/token/token.go)

---

## 1.3. The Lexer

- goal: writing our own lexer
- input: source code
- output: tokens (represents the source code)
  - we do not need buffer, since we will repeatedly call `NextToken()` method
  - `NextToken()` outputs the next token

what we need to do:

- initialise the lexter with our source code
- repeatedly call `NextToken()` on it to go through the source code, token by token, char by char

situation (to make our life simpler):

- source code as "string"
  - note: be aware that the production version should include -> filenames, line numbers to token
  - to better track down lexing and parsing errors
  - can be better to initialise the lexer with an `io.Reader` and the `filename`
  - but we won't do that since it adds complexity, so we handle these simply as "string"

create new `package` with test first: [lexer/lexer_test.go](../gopher/lexer/lexer_test.go)

---

## 1.4. Extending Our Token Set and Lexer

- to eliminate the need to jump between packages when later writing our parser
  - need to extend our lexer so that it can recognise more of the Gopher language and output more tokens

TODO: add support for:

- `==`
- `!`
- `!=`
- `-`
- `/`
- `*`
- `<`
- `>`

and keywords:

- `true`
- `false`
- `if`
- `else`
- `return`

new tokens we need to add:

- one-character token (e.g. `-`)
- two-character token (e.g. `==`)
- keyword token (e.g. `return`)

be aware that "lexer"'s job is not to tell us whether:

- code makes sense
- code works
- code contains errors

thus, basically this can be also handle gibberish like `!-/*5`,
analysis codes semantically correct or not will be done after lexer.

for handling characters, length more than 2 (e.g. `!=`) we handle like this:

```gopher
case '!': // this case, we consider `!` and `!=`
	if l.peekChar() == '=' {
		ch := l.ch
		l.readChar()
		tok = token.Token{
			Type:    token.NOT_EQ,
			Literal: string(ch) + string(l.ch),
		}
	} else {
		tok = newToken(token.BANG, l.ch)
	}
```

- we save `l.ch` as `ch` in the local variable befor calling `l.readChar()
- this way, we don't lose current character and can safely advance the lexer
- so it leaves the `NextToken()`

---

## 1.5. Start of a REPL

the Gopher language needs a REPL (Read Eval Print Loop).
can easily see other interpreted languages:

- e.g. Python has REPL
- Ruby
- every JavaScript runtime
- most Lisps
- etc.

sometimes, REPL is called as:

- console
- interactive mode
- concept is the same:
  - Read: reads input
  - Eval: sends it to the interpreter for evaluation
  - Print: prints the result/output of the interpreter
  - Loop: start again
  
a REPL that tokenizes Gopher course code: [repl/repl.go](../gopher/repl/repl.go)

---
